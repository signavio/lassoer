#!/usr/bin/env python3
from optparse import OptionParser
from pathlib import Path
from dotenv import dotenv_values
import logging

logging.basicConfig(level=logging.INFO)
parser = OptionParser(
    #   EXAMPLES:
    #      > load -t sqlite -o sqlite.db accounts.csv leads.csv
    #      > load -t postgresql -s acme_2020-07-testDrive_sap-O2C \\
    #        -f ~/dtools.env bkpf.csv bseg.csv
    usage="%prog [options] file...\nLoad CSV files into a sql database",
    version="%prog v0.3.0"
)
parser.add_option("-d", "--drop-schema", dest="drop_schema", action="store_true",
                  help="Drop existing database schema before create")
parser.add_option("-e", "--environment-file", dest="environment_file",
                  help="Location of environment file containing credentials",
                  default=Path('.') / '.env')
parser.add_option("-s", "--db-schema", dest="db_schema", type="string",
                  help="The database schema to load .csv files into")
parser.add_option("-t", "--type", dest="db_type", type="string", default="postgresql",
                  help="Type of sql database (postgresql or athena)")
(options, args) = parser.parse_args()
env = dotenv_values(options.environment_file)

def load_into_postgresql(env, create_stmt):
    from sqlalchemy import create_engine, text
    from os.path import basename, splitext
    db_uri = f'postgres://{env["PSQL_USERNAME"]}:{env["PSQL_PASSWORD"]}' +\
    f'@{env["PSQL_HOST"]}/{env["PSQL_DB"]}'
    (table_name, _) = splitext(basename(basename(args[0])))
    engine = create_engine(db_uri)
    conn = engine.connect()
    if options.drop_schema:
        drop_schema_then_create = text(f'\
        DROP SCHEMA IF EXISTS {options.db_schema} CASCADE;\
        CREATE SCHEMA {options.db_schema};')
        conn.execute(drop_schema_then_create)
    if options.db_schema:
        conn.execute(f'SET search_path TO {options.db_schema};')
    create_stmt = f'CREATE TABLE IF NOT EXISTS {table_name} ( ' + create_stmt
    conn.execute(create_stmt)
    psql_copy(db_uri, options.db_schema, args[0])

def create_stmt_ddl_from(csv_metadata_file):
    import csv
    create_stmt = ""
    with open(csv_metadata_file, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        first_row = next(reader)
        create_stmt = f'{first_row["column_name"]} {first_row["data_type"]},'
        for row in reader:
            create_stmt += f' {row["column_name"]} {row["data_type"]},'
        create_stmt = create_stmt[:-1] + ')'
    return create_stmt

def psql_copy(db_uri, db_schema, csv_file):
   from os.path import basename, splitext
   from subprocess import Popen, PIPE
   (table_name, _) = splitext(basename(csv_file))
   psql_copy_cmd = [
    'psql', db_uri,
    '-c', f'\COPY "{db_schema}".{table_name} ' +\
    f'FROM {csv_file} WITH ' +\
    "DELIMITER AS '\t' " +\
    "NULL AS '' " +\
    "CSV HEADER " +\
    "QUOTE AS '\"' " +\
    "ESCAPE AS '\"'"
    ]
   process = Popen(psql_copy_cmd, stdout=PIPE, stderr=PIPE)
   stdout, stderr = process.communicate()
   if len(stdout) > 0:
       rows_copied = stdout.decode("utf-8")[:-1] 
       print(f'{rows_copied} rows from {csv_file} to "{db_schema}".{table_name}')
   if len(stderr) > 0:
       print(stderr.decode("utf-8")[:-1])

def load_into_athena(env, create_stmt):
    import boto3
    from botocore.config import Config
    from os.path import basename, splitext
    import re
    aws_session = boto3.Session(
        region_name = 'eu-central-1',
        aws_access_key_id = env["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key = env["AWS_SECRET_ACCESS_KEY"]
    )
    csv_file = basename(args[0])
    (table_name, _) = splitext(csv_file)
    s3 = aws_session.resource('s3')
    s3.meta.client.upload_file(args[0], env['BUCKET_NAME'], f"{table_name}/{csv_file}")
    client = aws_session.client('athena')
    query = f"CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (" +\
            re.sub("VARCHAR", "string", create_stmt) + " " +\
            "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' " +\
            f"LOCATION 's3://{env['BUCKET_NAME']}/{table_name}/' " +\
            'TBLPROPERTIES ("skip.header.line.count"="1")'
    response = client.start_query_execution(
        QueryString=query,
        QueryExecutionContext={
            'Database': env['ATHENA_NAME']
        },
        ResultConfiguration={
            'OutputLocation': f's3://{env["BUCKET_NAME"]}/logs/',
            'EncryptionConfiguration': {
                'EncryptionOption': 'SSE_S3'
            }
        },
        WorkGroup=env['ATHENA_WORKGROUP']
    )
    print(response)

if options.db_type == "postgresql":
    create_stmt = create_stmt_ddl_from(f"{args[0]}.metadata")
    load_into_postgresql(env, create_stmt)
elif options.db_type == "athena":
    create_stmt = create_stmt_ddl_from(f"{args[0]}.metadata")
    load_into_athena(env, create_stmt)
else:
    print(f"The database of type `{options.db_type}` is not supported")
